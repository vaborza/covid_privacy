# Victor Borza
# Jul 30, 2020
# Read the Johns-Hopkins dataset and evaluate for reidentification risk

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import comb
import seaborn as sns
import random
import plotly.figure_factory as ff
import plotly.express as px
import time
import multiprocessing as mp

rng = np.random.default_rng()

# Apply the Golle formula, log must be used to prevent overflows
def estimate_anon(pop,bins,k_level):
    i = 1
    return sum(np.exp(np.log(comb(pop,i,exact=False)) + \
    np.log(bins) * (1-pop) + np.log(i) + \
    np.log((bins-1)) * (pop-i)) for i in range(1,k_level+1))

###

def gen_risk_ratio(sex_bins,race_bins,ethnicity_bins,age_specificity,age_cap,k_level):
    
    # Generate heatmap over time for 50 randomly selected FIPS codes
    # sex_bins: Bins by sex
    # race_bins: Bins by race 
    # ethnicity_bins: Bins by ethnicity
    # age_specificity: In years, with 1 being defined to the year
    # age_cap: Maximum age before we group all together
    # k_level: Set size of bin desired to be considered anonymous (i.e. we consider anonymity
    #          if a positive individual shares selected data with k-1 individuals in the bin)

    df = pd.read_csv('../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')
    df.drop(['UID','iso2','iso3','code3','Combined_Key','Admin2','Province_State','Country_Region',
        'Lat','Long_'], axis=1,inplace=True)

    df.set_index('FIPS',inplace=True)

    # Calculate the rolling average using a 7-day window
    df_rolled = df.rolling(7,axis=1,center=False).mean()

    # Generate age bins from specified age values, then calculate total number of bins
    age_bins = np.rint(age_cap / age_specificity) + 1 #Round to integer, then add 1 for the 'and up' case
    total_bins = sex_bins * race_bins * ethnicity_bins * age_bins

    # Generate estimate of risk as a proportion of population that is in a bin of size k or less
    df_at_risk = estimate_anon(df_rolled,total_bins,k_level)

    # Remove leading 0's generated by the rolling average function
    df_at_risk.replace(0,np.nan,inplace=True)
    return df_at_risk / df_rolled

###

def gen_heatmap(seed=7312020,sex_bins=2,race_bins=7,ethnicity_bins=2,
        age_specificity=1,age_cap=90,k_level=1):
    
    # Generate heatmap over time for 50 randomly selected FIPS codes, specify seed if desired
    # Inputs map to gen_risk_ratio, defaults are based on OMB data

    random.seed(seed)

    df_risk_ratio = gen_risk_ratio(sex_bins,race_bins,ethnicity_bins,age_specificity,age_cap,k_level)
    dates = pd.to_datetime(df_risk_ratio.columns.values)
    loc_pool = random.sample(range(len(df_risk_ratio)),50)

    ax=sns.heatmap(df_risk_ratio.iloc[loc_pool].to_numpy(),cmap='jet')
    plt.show()

###

def threshold_re_id(sex_bins=2,race_bins=7,ethnicity_bins=2,age_specificity=1,
        age_cap=90,k_level=1,id_cutoff=0.05):

    # For a given sharing profile, estimate the number of counties that can safely share that data
    # using a desired k level and population percentage
    # id_cutoff: The percentage of the population that is acceptable to be identifiable at the given
    #            k level or less

    df_risk_ratio = gen_risk_ratio(sex_bins,race_bins,ethnicity_bins,age_specificity,age_cap,k_level)
    
    # Identify counties where risk is less than the determined cutoff
    safe_locales = (df_risk_ratio < id_cutoff).to_numpy().sum(axis=0) / len(df_risk_ratio.index)
    
    fig, ax = plt.subplots()
    plt.plot(df_risk_ratio.columns.values,safe_locales)
    
    # Label by week
    for n, label in enumerate(ax.xaxis.get_ticklabels()):
        if n % 7 != 0:
            label.set_visible(False)
    plt.xticks(rotation=60)
    plt.title('Proportion of counties with an expected re-identification risk of ' + str(id_cutoff) +
            ' at k-level of ' + str(k_level))
    ax.annotate('# Sexes: ' + str(sex_bins) + '\n# Races: ' + str(race_bins) +
            '\n# Ethnicities: ' + str(ethnicity_bins) + '\nAge Specificity: ' +
            str(age_specificity) + '\nAge Cap: ' + str(age_cap),xy=(0.2,0.7),xycoords='figure fraction')
    plt.show()

###

def gen_geomap(sex_bins=2,race_bins=7,ethnicity_bins=2,age_specificity=1,age_cap=90,k_level=1,
        date='7/29/20'):

    # Show COVID re-identification risk for the US (as a choropleth map) on a given date
    # date: string input (no leading 0's) of the desired date
    
    df_risk_ratio = gen_risk_ratio(sex_bins,race_bins,ethnicity_bins,age_specificity,age_cap,k_level)
    
    # Strip out all values of NaN either in the FIPS code or COVID cases, that way it shows up white
    df_risk_ratio = df_risk_ratio[df_risk_ratio.index.notna() & df_risk_ratio[date].notna()]

    fips = (df_risk_ratio.index.values)
    values = df_risk_ratio[date]
    
    # N.B. Plasma is perceptually linear but the chosen endpoints are not, as data are more useful
    # at the extremes
    cmap = px.colors.sequential.Plasma
    endpts = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]

    fig = ff.create_choropleth(fips=fips,values=values,colorscale=cmap,round_legend_values=False,
            title=('COVID Data Sharing Risk on ' + date + ' at k-level ' + str(k_level) +
            '<br />Using: ' + str(sex_bins) + ' sexes, ' + str(race_bins) + ' races, ' + 
            str(ethnicity_bins) + ' ethnicities, Age in ' + str(age_specificity) + 
            '-year bins, until ' + str(age_cap) + ' years'), 
            binning_endpoints = endpts, legend_title='Re-ID risk')
    fig.layout.template = None
    fig.show()

###

def make_census_array(df,FIPS):
    
    # Turn census data for a FIPS region into a single 5-dimensional array
    #
    # Inputs:
    # df: the trimmed census dataframe for the desired FIPS
    #
    # Outputs:
    # pop_arrays: an 18x2x6x2 array containing the counts in the region by
    #             age, ethnicity, race, and sex in that order - key below
    # age index (0-17): 0-4,5-9,10-14,15-19,20-24,25-29,30-34,35-39,40-44,45-49,50-54,55-59,
    #                    60-64,65-69,70-74,75-79,80-84,85+
    # ethnicity index (0-1): 0 non-hispanic, 1 hispanic
    # race index (0-5): 0 white, 1 black, 2 Am. Indian/AK native, 3 asian, 4 native hawaiian/OPI,
    #                   5 two or more races
    # sex index (0-1): 0 male, 1 female
    #
    # Example: pop_arrays(4,0,0,0) is the number of 20-24 yr old white non-hispanic males

    df = df[df['FIPS'] == FIPS]
    pop_df = df[df['AGEGRP'] != 0]
    pop_arrays = pop_df[['NHWA_MALE','NHWA_FEMALE','NHBA_MALE','NHBA_FEMALE','NHIA_MALE',
        'NHIA_FEMALE','NHAA_MALE','NHAA_FEMALE','NHNA_MALE','NHNA_FEMALE','NHTOM_MALE',
        'NHTOM_FEMALE','HWA_MALE','HWA_FEMALE','HBA_MALE','HBA_FEMALE','HIA_MALE','HIA_FEMALE',
        'HAA_MALE','HAA_FEMALE','HNA_MALE','HNA_FEMALE','HTOM_MALE',
        'HTOM_FEMALE']].to_numpy()

    pop_arrays = pop_arrays.reshape(432)

    return pop_arrays

###

def gen_mc_risk_ratio(case_df, census_df,k_level,FIPS):

    demo_arr = census_df.loc[census_df['FIPS'] == FIPS,['DEMO_ARR']].to_numpy()
    
    # Convert demographic array to 1-dimension
    demo_list = demo_arr[0,0][0].reshape(-1)

    # Find the time-series for the desired location
    case_series = case_df.xs(float(FIPS))
    anon_list = []

    for case in case_series:
        # Take a Monte Carlo sample of 'individuals' from the region population
        mc_sample = rng.choice(np.sum(demo_list),case,replace=False)
        
        # Identify the demographic bins that the sampled individuals fall in and total bins of size < k
        cum_demo = np.cumsum(demo_list)
        sample_bins = [np.argmax(cum_demo >= k) for k in mc_sample]
        anon_list.append(np.sum(demo_list[sample_bins] <= k_level))

    return(anon_list/case_series.to_numpy())

###

def monte_carlo_sim(k_level=1):

    ##### --- Build the Census Pickle --- #####

    #census_df = pd.read_pickle('../census_data/census_usa_2019.pkl')

    # Generate full 5-digit FIPS codes from FIPS state and county codes in census data
    #census_df['STATE'] = census_df['STATE'].astype(str).apply(lambda x: x.zfill(2))
    #census_df['COUNTY'] = census_df['COUNTY'].astype(str).apply(lambda x: x.zfill(3))
    #census_df['FIPS'] = census_df["STATE"].astype(str) + census_df["COUNTY"].astype(str)
    
    # Get the total population (based on 2019 census estimates) by FIPS code in a dict
    #pop_df = census_df.loc[census_df['AGEGRP'] == 0]
    #sample_df = pd.concat([pop_df['FIPS'],pop_df['TOT_POP']],axis=1)
    
    #sample_df['DEMO_ARR'] = sample_df['FIPS'].apply(lambda row: [make_census_array(census_df,row)])
    #sample_df.to_pickle('../census_data/processed_19_data.pkl')

    ##### --- End Build the Census Pickle --- #####

    start_t = time.time()
    sample_df = pd.read_pickle('../census_data/processed_19_data.pkl')

    case_df = pd.read_csv('../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')
    case_df.drop(['UID','iso2','iso3','code3','Combined_Key','Admin2','Province_State','Country_Region',
        'Lat','Long_'], axis=1,inplace=True)

    case_df.set_index('FIPS',inplace=True)

    # Calculate the rolling average using a 7-day window
    case_df_rolled = case_df.rolling(7,axis=1,center=False).mean()
    
    ##a = gen_mc_risk_ratio(case_df,sample_df,k_level,'01009')
    ##print(a)

    FIPScodes = case_df.index[case_df.index.isin(sample_df['FIPS'].astype(float))].astype(int).astype(str)
    
    FIPScodes = FIPScodes.to_numpy().astype(str)
    FIPScodes = np.chararray.zfill(FIPScodes,5)

    mc_risk = pd.DataFrame(index=FIPScodes,columns=case_df.columns)
    i = 0
    for fips_code in mc_risk.index:
        if i % 10 == 0:
            print('Finished wih #'+str(i))
        if i < 10000:
            mc_risk.loc[fips_code] = gen_mc_risk_ratio(case_df,sample_df,k_level,fips_code)
        else: break
        i += 1

    mc_risk.to_pickle('../data/mc_k_'+str(k_level)+'.pkl')

    print('--- %s seconds----' % (time.time() - start_t))

###

def gen_mc_geomap(k_level=1, date='7/29/20'):

    # Show COVID re-identification risk for the US (as a choropleth map) on a given date
    # date: string input (no leading 0's) of the desired date
    # 
    # Used w/ Monte Carlo risk simulation

    df_risk_ratio = pd.read_pickle('../data/mc_k_'+str(k_level)+'.pkl')
    
    # Strip out all values of NaN either in the FIPS code or COVID cases, that way it shows up white
    df_risk_ratio = df_risk_ratio[df_risk_ratio.index.notna() & df_risk_ratio[date].notna()]

    fips = (df_risk_ratio.index.values)
    values = df_risk_ratio[date]
    
    # N.B. Plasma is perceptually linear but the chosen endpoints are not, as data are more useful
    # at the extremes
    cmap = px.colors.sequential.Plasma
    endpts = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]

    fig = ff.create_choropleth(fips=fips,values=values,colorscale=cmap,round_legend_values=False,
            title=('COVID Data Sharing Risk on ' + date + ' at k-level ' + str(k_level)), 
            binning_endpoints = endpts, legend_title='Re-ID risk')
    fig.layout.template = None
    fig.show()

###

def threshold_mc_re_id(k_level=1,id_cutoff=0.05):

    # For a given sharing profile, estimate the number of counties that can safely share that data
    # using a desired k level and population percentage
    # id_cutoff: The percentage of the population that is acceptable to be identifiable at the given
    #            k level or less
    #
    # Used w/ Monte Carlo risk simulation

    df_risk_ratio = pd.read_pickle('../data/mc_k_'+str(k_level)+'.pkl')
    
    # Identify counties where risk is less than the determined cutoff
    safe_locales = (df_risk_ratio < id_cutoff).to_numpy().sum(axis=0) / len(df_risk_ratio.index)
    
    fig, ax = plt.subplots()
    plt.plot(df_risk_ratio.columns.values,safe_locales)
    
    # Label by week
    for n, label in enumerate(ax.xaxis.get_ticklabels()):
        if n % 7 != 0:
            label.set_visible(False)
    plt.xticks(rotation=60)
    plt.title('Proportion of counties with an expected re-identification risk of ' + str(id_cutoff) +
            ' at k-level of ' + str(k_level))
    plt.show()

###

def gen_mc_heatmap(seed=7312020,k_level=1):
    
    # Generate heatmap over time for 50 randomly selected FIPS codes, specify seed if desired
    # Used w/ Monte Carlo risk simulation

    random.seed(seed)

    df_risk_ratio = pd.read_pickle('../data/mc_k_'+str(k_level)+'.pkl')
    dates = pd.to_datetime(df_risk_ratio.columns.values)
    loc_pool = random.sample(range(len(df_risk_ratio)),50)

    ax=sns.heatmap(df_risk_ratio.iloc[loc_pool].to_numpy(),cmap='jet')
    plt.show()

###

def gen_mc_risk_ratio_opt(num):
    
    # Generates the bin size (k) for each COVID case using a Monte Carlo algorithm and returns
    # results as a pickled series of length 596980 (aka 190 dates x 3142 FIPS codes) containing
    # lists of various sizes
    # 
    # num is a dummy variable used for multiprocessing, so is the return value of 1

    start_t = time.time()
    sample_df = pd.read_pickle('../census_data/processed_19_data.pkl')

    case_df = pd.read_csv('../csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv')
    case_df.drop(['UID','iso2','iso3','code3','Combined_Key','Admin2','Province_State','Country_Region',
        'Lat','Long_'], axis=1,inplace=True)

    case_df.set_index('FIPS',inplace=True)

    # Calculate the rolling average using a 7-day window
    case_df_rolled = case_df.rolling(7,axis=1,center=False).mean()
    
    # Convert FIPS floats in the JHU dataset to strings with leading zeroes
    case_df = case_df[case_df.index.notnull()]
    case_df.index = case_df.index.astype(int).map(str).str.zfill(5)

    # Trim the JHU dataset to only use FIPS that we have census data for
    case_df = case_df[case_df.index.isin(sample_df['FIPS'])]
    
    # Extra check to ensure both lists correspond to the same FIPS codes
    case_df.sort_index()
    sample_df.sort_values(by=['FIPS'])

    # Convert demographics to a 3142 [FIPS] x 432 [demographic bins] array, and assoc. cumulative array
    demo_arr = np.stack(sample_df['DEMO_ARR'].to_numpy()).squeeze()
    cum_demo_arr = np.cumsum(demo_arr,axis=1)
 
    case_arr = case_df.to_numpy()
    anon_list = []

    # Find the time-series for the desired location
    for i in range(case_arr.shape[0]): 
        if i % 100 == 0:
            print('Finished wih #'+str(i))
        if i > 100: break
        for case in case_arr[i][:]:
            # Take a Monte Carlo sample of 'individuals' from the region population
            mc_sample = rng.choice(np.sum(demo_arr[i][:]),case,replace=False)
            
            # Identify the demo bins that the sampled individuals fall in and total bins of size < k
            sample_bins = [np.argmax(cum_demo_arr[i][:] >= k) for k in mc_sample]
            anon_list.append(demo_arr[i][sample_bins])
    
    # Turn list into Series, then pickle
    mc_risk = pd.Series(anon_list)
    mc_risk.to_pickle('../data/mc_opt_'+str(time.time()).replace('.','-')+'.pkl')

    print('--- %s seconds----' % (time.time() - start_t))
    return(1)
###

# Execute function on desired number of workers, for 'x' repetitions 
if __name__ == '__main__':
    p = mp.Pool(16)
    p.map(gen_mc_risk_ratio_opt,range(2))

